plt02 <- top9nnetPred %>%
ggplot(aes(x = BiologicalMaterial06, y = Yield)) +
geom_point()+ geom_smooth(method = "lm") + theme_bw()
plt03 <- top9nnetPred %>%
ggplot(aes(x = ManufacturingProcess17, y = Yield)) +
geom_point() + geom_smooth(method = "lm") + theme_bw()
plt04 <- top9nnetPred %>%
ggplot(aes(x = BiologicalMaterial03, y = Yield)) +
geom_point() + geom_smooth(method = "lm") + theme_bw()
plt05 <- top9nnetPred %>%
ggplot(aes(x = ManufacturingProcess36, y = Yield)) +
geom_point() + theme_bw()
plt06 <- top9nnetPred %>%
ggplot(aes(x = BiologicalMaterial02, y = Yield)) +
geom_point() + geom_smooth(method = "lm") + theme_bw()
plt07 <- top9nnetPred %>%
ggplot(aes(x = ManufacturingProcess09, y = Yield)) +
geom_point() + geom_smooth(method = "lm") + theme_bw()
plt08 <- top9nnetPred %>%
ggplot(aes(x = BiologicalMaterial12, y = Yield)) +
geom_point() + geom_smooth(method = "lm") + theme_bw()
plt09 <- top9nnetPred %>%
ggplot(aes(x = ManufacturingProcess06, y = Yield)) +
geom_point() + geom_smooth(method = "lm") + theme_bw()
# Biological Predictors
grid.arrange(plt02, plt04, plt06, plt08, nrow = 2, top = "BIOLOGICAL PREDICTORS")
# Process Predictors
grid.arrange(plt01, plt03, plt05, plt07, plt09,
nrow = 2, top = "PROCESS PREDICTORS")
postResample(predict(rpartTree, testData), testData$Yield)
postResample(predict(rpartTree, testData), testData$Yield)
cubistTuned <- train(Yield~., data = trainData, method = "cubist")
postResample(predict(rpartTree, testData), testData$Yield)
postResample(predict(rpartTree, testData), testData$Yield)
library(rpart)
library(partykit)
rpartTree <- rpart(Yield ~., data = trainData)
library(rpart)
rpartTree <- rpart(Yield ~., data = trainData)
rfModel <- randomForest(Yield~., data = trainData,
importance = TRUE,
ntree = 1000)
gbmModel <- train(Yield ~., data = trainData, method = "gbm",
verbose = FALSE)
postResample(predict(rpartTree, testData), testData$Yield)
postResample(predict(rfModel, testData), testData$Yield)
postResample(predict(gbmModel, testData), testData$Yield)
postResample(predict(cubistTuned, testData), testData$Yield)
postResample(predict(rpartTree, testData), testData$Yield)
postResample(predict(rfModel, testData), testData$Yield)
postResample(predict(gbmModel, testData), testData$Yield)
postResample(predict(cubistTuned, testData), testData$Yield)
postResample(predict(rpartTree, testData), testData$Yield)
postResample(predict(rfModel, testData), testData$Yield)
postResample(predict(gbmModel, testData), testData$Yield)
postResample(predict(cubistTuned, testData), testData$Yield)
cubistTuned <- train(Yield~., data = trainData, method = "cubist")
knitr::opts_chunk$set(echo=TRUE, warning=FALSE,
message=FALSE,
collapse = FALSE,
comment = "#>" )
library(webshot)
install_phantomjs(force = TRUE)
library(elasticnet)
library(caret)
library(MASS)
library(lars)
library(stats)
library(pls)
library(tidyverse)
library(dplyr)
library(RANN)
library(GGally)
library(naniar)
library(party)
library(Cubist)
library(gbm)
library(randomForest)
library(tibble)
library(AppliedPredictiveModeling)
data(ChemicalManufacturingProcess)
df_raw <- ChemicalManufacturingProcess
paste0("Missing Values: ",sum(is.na(ChemicalManufacturingProcess)))
gg_miss_var(df_raw)
preProcess_NAdata_model <- preProcess(df_raw, method ="medianImpute")
df <- predict(preProcess_NAdata_model, newdata = df_raw)
paste0(sum(is.na(df))," values missing after imputation")
# Create the training and test datasets
set.seed(100)
# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(df$Yield, p=0.8, list=FALSE)
# Step 2: Create the training  dataset
trainData <- df[trainRowNumbers,]
# Step 3: Create the test dataset
testData <- df[-trainRowNumbers,]
preProcValues <- preProcess(trainData, method = c("center", "scale"))
trainTransformed <- predict(preProcValues, trainData)
testTransformed <- predict(preProcValues, testData)
xTrain <- trainTransformed %>% dplyr::select(-Yield)
yTrain <- trainTransformed %>% dplyr::select(Yield)
xTest  <- testTransformed  %>% dplyr::select(-Yield)
yTest  <- testTransformed %>% dplyr::select(Yield)
lm_model <- train(xTrain, yTrain$Yield, method="lm", trControl=trainControl(method="repeatedcv",repeats=5) )
lm_model
summary(lm_model)
plot(lm_model$finalModel)
finalLMPred <- predict(lm_model$finalModel, newdata = xTest)
## The function 'postResample' can be used to get the test set
## performance values
postResample(pred = finalLMPred, obs = yTest$Yield)
plot(caret::varImp(lm_model), top = 10)
caret::varImp(lm_model, scale = FALSE)
library(gridExtra)
top10lmPred <- trainTransformed %>%
select(Yield,
ManufacturingProcess32,
ManufacturingProcess33,
ManufacturingProcess28,
ManufacturingProcess37,
ManufacturingProcess13,
ManufacturingProcess07,
BiologicalMaterial05,
ManufacturingProcess04,
ManufacturingProcess16,
BiologicalMaterial11)
plt01 <- top10lmPred %>%
ggplot(aes(x = ManufacturingProcess32, y = Yield)) +
geom_point() + geom_smooth(method = "lm") + theme_bw()
plt02 <- top10lmPred %>%
ggplot(aes(x = ManufacturingProcess33, y = Yield)) +
geom_point()+ geom_smooth(method = "lm") + theme_bw()
plt03 <- top10lmPred %>%
ggplot(aes(x = ManufacturingProcess28, y = Yield)) +
geom_point() + geom_smooth(method = "lm") + theme_bw()
plt04 <- top10lmPred %>%
ggplot(aes(x =  ManufacturingProcess37, y = Yield)) +
geom_point() + geom_smooth(method = "lm") + theme_bw()
plt05 <- top10lmPred %>%
ggplot(aes(x = ManufacturingProcess13, y = Yield)) +
geom_point() + geom_smooth(method = "lm") + theme_bw()
plt06 <- top10lmPred %>%
ggplot(aes(x = ManufacturingProcess07, y = Yield)) +
geom_point() + geom_smooth(method = "lm") + theme_bw()
plt07 <- top10lmPred %>%
ggplot(aes(x = BiologicalMaterial05, y = Yield)) +
geom_point() + geom_smooth(method = "lm") + theme_bw()
plt08 <- top10lmPred %>%
ggplot(aes(x = ManufacturingProcess04, y = Yield)) +
geom_point() + geom_smooth(method = "lm") + theme_bw()
plt09 <- top10lmPred %>%
ggplot(aes(x = ManufacturingProcess16, y = Yield)) +
geom_point() + geom_smooth(method = "lm") + theme_bw()
plt10 <- top10lmPred %>%
ggplot(aes(x = BiologicalMaterial11, y = Yield)) +
geom_point() + geom_smooth(method = "lm") + theme_bw()
# Biological Predictors
grid.arrange(plt07, plt10, nrow = 1, top = "BIOLOGICAL PREDICTORS")
# Process Predictors
grid.arrange(plt01, plt02, plt03, plt04, plt05, plt06, plt08, plt09,
nrow = 3, top = "PROCESS PREDICTORS")
library(caret)
library(mlbench)
set.seed(200)
trainingData <- mlbench.friedman1(200, sd = 1)
## We convert the 'x' data from a matrix to a data frame
## One reason is that this will give the columns names.
trainingData$x <- data.frame(trainingData$x)
## Look at the data using
caret::featurePlot(trainingData$x, trainingData$y)
## or other methods.
## This creates a list with a vector 'y' and a matrix
## of predictors 'x'. Also simulate a large test set to
## estimate the true error rate with good precision:
testData <- mlbench.friedman1(5000, sd = 1)
testData$x <- data.frame(testData$x)
library(caret)
knnModel <- train(x = trainingData$x,
y = trainingData$y,
method = "knn",
preProc = c("center", "scale"),
tuneLength = 10)
knnModel
knnPred <- predict(knnModel, newdata = testData$x)
## The function 'postResample' can be used to get the test set
## perforamnce values
postResample(pred = knnPred, obs = testData$y)
tooHigh <- findCorrelation(cor(trainingData$x), cutoff = .75)
tooHigh
# Resource: Chapter 7.5 of textbook "Applied Predictive Modeling" by KJ
## Create a specific candidate set of models to evaluate:
nnetGrid <- expand.grid(.decay = c(0, 0.01, .1),
.size = c(1:10),
.bag = FALSE)
# We can use caret to perform a grid search using 10-fold cross-validation.
# The train function generates a resampling estimate of performance. Because
# the training set size is not small, 10-fold cross-validation should produce
# reasonable estimates of model performance. The function trainControl specifies
# the type of resampling:
ctrl <- trainControl(method = "cv", number = 10)
nnetTune <- train(trainingData$x, trainingData$y,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = ctrl,
## Automatically standardize data prior to modeling
## and prediction
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,
MaxNWts = 10 * (ncol(trainingData$x) + 1) + 10 + 1,
maxit = 500)
nnetTune
nnetPred <- predict(nnetTune, newdata = testData$x)
## The function 'postResample' can be used to get the test set
## performance values
postResample(pred = nnetPred, obs = testData$y)
# Resource: Chapter 7.5 of textbook "Applied Predictive Modeling" by KJ
# Resource: http://uc-r.github.io/mars
library(earth)
library(dplyr)
## Create a specific candidate set of models to evaluate:
marsGrid <- expand.grid(degree = 1:3,
nprune = seq(2, 100, length.out = 10) %>% floor()
)
# We can use caret to perform a grid search using 10-fold cross-validation.
# The train function generates a resampling estimate of performance. Because
# the training set size is not small, 10-fold cross-validation should produce
# reasonable estimates of model performance. The function trainControl specifies
# the type of resampling:
ctrl <- trainControl(method = "cv", number = 10)
# cross validated model
tuned_mars <- train(
x = trainingData$x,
y = trainingData$y,
method = "earth",
metric = "RMSE",
trControl = ctrl,
tuneGrid = marsGrid
)
tuned_mars
ggplot(tuned_mars)
# best chosen MARS model
tuned_mars$bestTune
marsPred <- predict(tuned_mars, newdata = testData$x)
## The function 'postResample' can be used to get the test set
## performance values
postResample(pred = marsPred, obs = testData$y)
# Resource: Chapter 7.5 of textbook "Applied Predictive Modeling" by KJ
library(kernlab)
# We can use caret to perform a grid search using 10-fold cross-validation.
# The train function generates a resampling estimate of performance. Because
# the training set size is not small, 10-fold cross-validation should produce
# reasonable estimates of model performance. The function trainControl specifies
# the type of resampling:
ctrl <- trainControl(method = "cv", number = 10)
svmRTuned <- train(trainingData$x, trainingData$y,
method = "svmRadial",
preProc = c("center", "scale"),
tuneLength = 14,
trControl = ctrl)
svmRTuned
svmPred <- predict(svmRTuned, newdata = testData$x)
## The function 'postResample' can be used to get the test set
## performance values
postResample(pred = svmPred, obs = testData$y)
library(RANN)
library(dplyr)
library(naniar)
library(caret)
knnChemModel <- train(x = xTrain,
y = yTrain$Yield,
method = "knn",
preProc = c("center", "scale"),
tuneLength = 10)
knnChemModel
knnChemPred <- predict(knnChemModel, newdata = xTest)
## The function 'postResample' can be used to get the test set
## perforamnce values
postResample(pred = knnChemPred, obs = yTest$Yield)
tooHigh <- findCorrelation(cor(xTrain), cutoff = .75)
tooHigh
trainXnnet <- xTrain[, -tooHigh]
testXnnet  <- xTest[, -tooHigh]
# Resource: Chapter 7.5 of textbook "Applied Predictive Modeling" by KJ
## Create a specific candidate set of models to evaluate:
nnetGrid <- expand.grid(.decay = c(0, 0.01, .1),
.size = c(1:10),
.bag = FALSE)
# We can use caret to perform a grid search using 10-fold cross-validation.
# The train function generates a resampling estimate of performance. Because
# the training set size is not small, 10-fold cross-validation should produce
# reasonable estimates of model performance. The function trainControl specifies
# the type of resampling:
ctrl <- trainControl(method = "cv", number = 10)
nnetChemTune <- train(xTrain, yTrain$Yield,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = ctrl,
## Automatically standardize data prior to modeling
## and prediction
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,
MaxNWts = 10 * (ncol(trainXnnet) + 1) + 10 + 1,
maxit = 500)
nnetChemTune
nnetChemPred <- predict(nnetChemTune, newdata = xTest)
## The function 'postResample' can be used to get the test set
## performance values
postResample(pred = nnetChemPred, obs = yTest$Yield)
# Resource: Chapter 7.5 of textbook "Applied Predictive Modeling" by KJ
# Resource: http://uc-r.github.io/mars
library(earth)
library(dplyr)
## Create a specific candidate set of models to evaluate:
marsGrid <- expand.grid(degree = 1:3,
nprune = seq(2, 100, length.out = 10) %>% floor()
)
# We can use caret to perform a grid search using 10-fold cross-validation.
# The train function generates a resampling estimate of performance. Because
# the training set size is not small, 10-fold cross-validation should produce
# reasonable estimates of model performance. The function trainControl specifies
# the type of resampling:
ctrl <- trainControl(method = "cv", number = 10)
# cross validated model
tuned_ChemMARS <- train(
x = xTrain,
y = yTrain$Yield,
method = "earth",
metric = "RMSE",
trControl = ctrl,
tuneGrid = marsGrid
)
tuned_ChemMARS
marsChemPred <- predict(tuned_ChemMARS, newdata = xTest)
## The function 'postResample' can be used to get the test set
## performance values
postResample(pred = marsChemPred, obs = yTest$Yield)
# Resource: Chapter 7.5 of textbook "Applied Predictive Modeling" by KJ
library(kernlab)
# We can use caret to perform a grid search using 10-fold cross-validation.
# The train function generates a resampling estimate of performance. Because
# the training set size is not small, 10-fold cross-validation should produce
# reasonable estimates of model performance. The function trainControl specifies
# the type of resampling:
ctrl <- trainControl(method = "cv", number = 10)
svmChemRTuned <- train(xTrain, yTrain$Yield,
method = "svmRadial",
preProc = c("center", "scale"),
tuneLength = 14,
trControl = ctrl)
svmChemRTuned
svmChemPred <- predict(svmChemRTuned, newdata = xTest)
## The function 'postResample' can be used to get the test set
## performance values
postResample(pred = svmChemPred, obs = yTest$Yield)
plot(caret::varImp(nnetChemTune), top = 10)
caret::varImp(nnetChemTune)
library(gridExtra)
top9nnetPred <- trainTransformed %>%
select(Yield,
ManufacturingProcess13,
BiologicalMaterial06,
ManufacturingProcess17,
BiologicalMaterial03,
ManufacturingProcess36,
BiologicalMaterial02,
ManufacturingProcess09,
BiologicalMaterial12,
ManufacturingProcess06)
plt01 <- top9nnetPred %>%
ggplot(aes(x = ManufacturingProcess13, y = Yield)) +
geom_point() + geom_smooth(method = "lm") + theme_bw()
plt02 <- top9nnetPred %>%
ggplot(aes(x = BiologicalMaterial06, y = Yield)) +
geom_point()+ geom_smooth(method = "lm") + theme_bw()
plt03 <- top9nnetPred %>%
ggplot(aes(x = ManufacturingProcess17, y = Yield)) +
geom_point() + geom_smooth(method = "lm") + theme_bw()
plt04 <- top9nnetPred %>%
ggplot(aes(x = BiologicalMaterial03, y = Yield)) +
geom_point() + geom_smooth(method = "lm") + theme_bw()
plt05 <- top9nnetPred %>%
ggplot(aes(x = ManufacturingProcess36, y = Yield)) +
geom_point() + theme_bw()
plt06 <- top9nnetPred %>%
ggplot(aes(x = BiologicalMaterial02, y = Yield)) +
geom_point() + geom_smooth(method = "lm") + theme_bw()
plt07 <- top9nnetPred %>%
ggplot(aes(x = ManufacturingProcess09, y = Yield)) +
geom_point() + geom_smooth(method = "lm") + theme_bw()
plt08 <- top9nnetPred %>%
ggplot(aes(x = BiologicalMaterial12, y = Yield)) +
geom_point() + geom_smooth(method = "lm") + theme_bw()
plt09 <- top9nnetPred %>%
ggplot(aes(x = ManufacturingProcess06, y = Yield)) +
geom_point() + geom_smooth(method = "lm") + theme_bw()
# Biological Predictors
grid.arrange(plt02, plt04, plt06, plt08, nrow = 2, top = "BIOLOGICAL PREDICTORS")
# Process Predictors
grid.arrange(plt01, plt03, plt05, plt07, plt09,
nrow = 2, top = "PROCESS PREDICTORS")
library(mlbench)
set.seed(200)
simulated <- mlbench.friedman1(200, sd = 1)
simulated <- cbind(simulated$x, simulated$y)
simulated <- as.data.frame(simulated)
colnames(simulated)[ncol(simulated)] <- "y"
head(simulated)
set.seed(600)
model1 <- randomForest(y ~., data = simulated, importance = TRUE, ntree = 1000) # model
rfImp1 <- varImp(model1, scale = FALSE) # Random Forest Importance Scores
rfImp1 <- tibble::rownames_to_column(rfImp1, "Predictors")
rfImp1
rfImp1[order(-rfImp1$Overall),]
# Visualization for original models' scores
rfImp1 %>%
mutate(name = fct_reorder(Predictors, Overall)) %>%
ggplot(aes(x = name, y = Overall)) +
geom_segment(aes(x=Predictors, xend=Predictors, y=min(Overall), yend= Overall), color = "grey") +
geom_point(color = "orange", size = 4) +
xlab("Model 1 Predictor Importance") +
theme_light() + theme_classic() +
coord_flip()
# From book
simulated$duplicate1 <- simulated$V1 + rnorm(200) * .1
cor(simulated$duplicate1, simulated$V1)
model2 <- randomForest(y~., data = simulated,  importance = T, ntree = 1000)
rfImp2 <- varImp(model2, scale = F)
Prednames <- c("V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8", "V9", "V10", "NEW")
rfImp2$Variables <- factor(rownames(rfImp2), levels = Prednames)
rfImp2[order(-rfImp2$Overall),]
set.seed(555)
cforest1 <- cforest(y~., data = simulated[,1:11], controls = cforest_control(ntree = 1000))
set.seed(444)
cforest2 <- cforest(y~., data = simulated[,1:11], controls = cforest_control(ntree = 1000))
# Variable Importances
cfImp1 <- varimp(cforest1)
cfImp2 <- varimp(cforest2)
cfImp1_cond <- varimp(cforest1, conditional = T)
cfImp2_cond <- varimp(cforest2, conditional = T)
# Comparing performances with conditional and non conditional random forests
cfImp1 <- data.frame(Original = cfImp1, Variable = factor(names(cfImp1), levels = Prednames))
cfImp2 <- data.frame(Extra = cfImp2, Variable = factor(names(cfImp2), levels = Prednames))
cfImp1_cond <- data.frame(CondInf = cfImp1_cond, Variable = factor(names(cfImp1_cond), levels = Prednames))
cfImp2_cond <- data.frame(Extra_CondInf = cfImp2_cond, Variable = factor(names(cfImp2_cond), levels = Prednames))
cfImp1[order(-cfImp1$Original),]
cfImp2[order(-cfImp2$Extra),]
cfImp1_cond[order(-cfImp1_cond$CondInf),]
cfImp2_cond[order(-cfImp2_cond$Extra_CondInf),]
# Boosted Trees: original
# Specifying t-distribution
# Having difficulty, might omit
# gbmModel <- gbm(y~., data = simulated, distribution = "tdist")
#
# booImp1 <- varImp(gbmModel)
# Trying bagging method
set.seed(777)
bagfit1 <- ipred::bagging(y~., data = simulated[,1:11], nbag = 60)
bag1Imp <- varImp(bagfit1)
bag1Imp$Variable <- factor(rownames(bag1Imp), levels = Prednames)
bag1Imp[order(-bag1Imp$Overall),]
set.seed(888)
bagfit2 <- ipred::bagging(y~., data = simulated[,1:11], nag = 60)
bag2Imp <- varImp(bagfit2)
bag2Imp$Variable <- factor(rownames(bag2Imp), levels = Prednames)
bag2Imp[order(-bag2Imp$Overall),]
# Cubist: original
set.seed(222)
cbfit1 <- cubist(x = simulated[, 1:10], y = simulated$y, committees = 100)
cbImp1 <- varImp(cbfit1)
names(cbImp1) <- "Original"
cbImp1$Variable <- factor(rownames(cbImp1), levels = Prednames)
cbImp1[order(-cbImp1$Variable),]
# Cubist: extra
set.seed(111)
cbfit2 <- cubist(x = simulated[ , names(simulated) !="y"], y = simulated$y, committees = 100)
cbImp2 <- varImp(cbfit2)
names(cbImp1) <- "Extra"
cbImp2$Variable <- factor(rownames(cbImp2), levels = Prednames)
cbImp2[order(-cbImp2$Variable),]
set.seed(222)
x1 <- rep(1:2, each = 450)
y <- x1 + rnorm(450, mean = 0, sd = 1)
set.seed(111)
x2 <- rnorm(450, mean = 0, sd = 2)
simulated_data <- data.frame(Y = y, X1 = x1, X2 = x2)
plot(simulated_data$Y ~ simulated_data$X1 + simulated_data$X2)
library(rpart)
rpartTree <- rpart(Yield ~., data = trainData)
rfModel <- randomForest(Yield~., data = trainData,
importance = TRUE,
ntree = 1000)
gbmModel <- train(Yield ~., data = trainData, method = "gbm",
verbose = FALSE)
cubistTuned <- train(Yield~., data = trainData, method = "cubist")
postResample(predict(rpartTree, testData), testData$Yield)
head(testData)
#histograms
par(mfrow = c(3,3))
for(i in 2:ncol(rawData)) {#distribution of each variable
hist(rawData[[i]], main = colnames(rawData[i]), col = "skyblue")
}
#libraries
library(kableExtra)
library(tidyverse)
library(tidymodels)
install.packages("tidymodels")
#libraries
library(kableExtra)
library(tidyverse)
library(tidymodels)
install.packages(c("callr", "farver", "generics", "lme4", "processx", "Rcpp", "rlang", "stringi"))
